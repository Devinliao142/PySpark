{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I/O to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "flight = spark.read.csv('/user/peizhou.liao/giraph/data/Airports2.csv', sep=',', header=True)\n",
    "\n",
    "flight.write.save('/user/peizhou.liao/giraph/data/Airports_new.csv', format='csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display some rows\n",
    "flight.show(5)  # flight.show(5, False) full display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the count of null values\n",
    "flight.filter(flight['Seats'].isNull()).count()\n",
    "\n",
    "# display a sinlge column\n",
    "flight.select('Origin_city').show()\n",
    "\n",
    "# filter rows\n",
    "flight.where(flight['Origin_airport']=='ATL')\n",
    "flight.where(F.col('Origin_airport').isNotNull())\n",
    "\n",
    "# filter rows and select some columns\n",
    "flight.where(flight['Origin_airport']=='ATL').select(['Origin_city', 'Destination_city'])\n",
    "\n",
    "# concatenate dataframe\n",
    "flight_dup = flight.union(flight)\n",
    "\n",
    "# keep distinct rows\n",
    "flight_uniq = fligh_dup.distinct() \n",
    "flight_airport = flight.select('Origin_airport').distinct()\n",
    "\n",
    "# sort dataframe by column\n",
    "flight.sort(F.col('Seats').desc()).show(n=10)\n",
    "\n",
    "# groupby and aggregate\n",
    "flight.groupBy('Origin_airport').agg(F.sum('Seats'), F.countDistinct('Destination_airport'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample 10% of rows without replacement\n",
    "flight_sample = flight.sample(False, 0.1, seed=11)  # flight.sample(True, 0.1, seed=11) with replacement\n",
    "\n",
    "# count frequency\n",
    "flight.cube('Origin_airport').count()\n",
    "\n",
    "# compute correlation\n",
    "flight.stat.corr('Passengers', 'Seats')\n",
    "\n",
    "# compute percentiles\n",
    "flight.approxQuantile('Passengers', [0.01, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.99], 0.01)\n",
    "\n",
    "# merge two dataframes\n",
    "new_data = data.join(traffic, on=['maid', 'dt'], how='left_outer')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- UDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts2date = F.udf(\n",
    "    lambda x: x.split()[0],\n",
    "    returnType=StringType()\n",
    ")\n",
    "\n",
    "# UDF with error handling\n",
    "def ts2hour(s):\n",
    "    try:\n",
    "        res = int(s.split()[1].split(':')[0])\n",
    "    except:\n",
    "        res = None\n",
    "    return res  \n",
    "ts2Hour = F.udf(\n",
    "    lambda x: ts2hour(x),\n",
    "    returnType=IntegerType()\n",
    ")\n",
    "\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col, udf\n",
    "\n",
    "df = <some pyspark dataframe with a column of feature_vector with the data type of pyspark.ml.linalg.SparseVector>\n",
    "model_weights = <just your typical numpy array>\n",
    "\n",
    "def get_weighted_sum(model_weights, user_feature_vector):\n",
    "    return user_feature_vector.dot(model_weights) / user_feature_vector.norm(1)\n",
    "\n",
    "def get_weighted_sum_udf(model_weights):\n",
    "    return udf(lambda feature_vector: get_weighted_sum(model_weights, feature_vector), DoubleType())\n",
    "\n",
    "new_df = df.select(get_weighted_sum_udf(model_weights)(col('feature_vector')).alias('weighted_sum'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Useful operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract elements from an array of struct\n",
    "df = df.withColumn('idType', df['ids'].getItem(0).idType).withColumn('idValue', df['ids'].getItem(0).idValue)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two ways to convert an RDD to DF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toDF()\n",
    "from pyspark.sql.types import Row\n",
    "#here you are going to create a function\n",
    "def f(x):\n",
    "    d = {}\n",
    "    for i in range(len(x)):\n",
    "        d[str(i)] = x[i]\n",
    "    return d\n",
    "#Now populate that\n",
    "df = rdd.map(lambda x: Row(**f(x))).toDF()\n",
    "\n",
    "# createDataFrame(rdd, schema)\n",
    "schema = StructType([StructField(str(i), StringType(), True) for i in range(32)])\n",
    "df = sqlContext.createDataFrame(rdd, schema)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
